

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Introduction &mdash; RHIPE 0.65.3 documentation</title>
    
    <link rel="stylesheet" href="_static/haiku.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/print.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.65.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/theme_extras.js"></script>
    <link rel="top" title="RHIPE 0.65.3 documentation" href="index.html" />
    <link rel="next" title="Airline Dataset" href="airline.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
  </head>
  <body>
      <div class="header"><h1 class="heading"><a href="index.html">
          <span>RHIPE 0.65.3 documentation</span></a></h1>
        <h2 class="heading"><span>Introduction</span></h2>
      </div>
      <div class="topnav">
      
        <p>
        «&#160;&#160;<a href="installation.html">Installation</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="airline.html">Airline Dataset</a>&#160;&#160;»
        </p>

      </div>
      <div class="content">
        
        
  <div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>Massive data sets have become commonplace today. Powerful hardware is readily
available with a terabyte of hard drive storage costing less than $150 and
computers with many cores a norm. Today, the moderately adventurous scientist
can connect two computers to form a distributed computing platform. Languages
and software tools have made concurrent and distributed computing accessibly to
the statistician.</p>
<p>It is important to stress that a massive data set is not just a single massive
entity that needs to be stored across multiple hard drives but rather the size
of the data created during the steps of an analysis. A &#8216;small&#8217; 14 GB data set
can easily become 190 GB as new data structures are created, or where multiple
subsets /transformations are each saved as different data sets. Large data sets
can come as they are or grow big because of the nature of the analysis. No analyst
wants her research to be restricted because the computing infrastructure cannot
keep up with the size or complexity.</p>
<div class="section" id="hadoop">
<h2>Hadoop<a class="headerlink" href="#hadoop" title="Permalink to this headline">¶</a></h2>
<p>Hadoop is an open source programming framework for distributed computing with
massive data sets using a cluster of networked computers. It has changed the way
many web companies work, bringing cluster computing to people with little
knowledge of the intricacies of concurrent/distributed programming. Part of the
reason for its success is that it has a fixed programming paradigm. It somewhat
restricts what the user can parallelize but once an algorithm has been written
the &#8216;MapReduce way&#8217;, concurrency and distribution over a cluster comes for free.</p>
<p>It consists of two components: the Hadoop Distributed Filesystem and Hadoop
MapReduce. They are based on the Google Filesystem and Google MapReduce
respectively. Companies using these include Amazon, Ebay, New York Times,
Facebook to name a few. The software can be downloaded from <a class="reference external" href="http://hadoop.apache.org/">here</a> .</p>
</div>
<div class="section" id="hadoop-distributed-filesystem">
<h2>Hadoop Distributed Filesystem<a class="headerlink" href="#hadoop-distributed-filesystem" title="Permalink to this headline">¶</a></h2>
<p>The Hadoop Distributed Filesystem (HDFS) sits on top of the file system of a
computer (called the local filesystem). It pools the hard drive space of a
cluster or heterogenous computers (e.g. different hardware and operating
systems) and provides a unified view to the user. For example, with a cluster of
10 computers each with 1TB hard drive space available to Hadoop, the HDFS
provides a user 10 TB of hard drive space. A single file can be bigger than
maximum size on the local filesystem e.g. 2TB files can be saved on the
HDFS. The HDFS is catered to large files and high throughput reads. Appends to
files are not allowed. Files written to the HDFS are chunked into blocks, each
block is replicated and saved on different cluster computers. This provides a
measure of safety in case of transient or permanent computer failures.  When a
file is written to the HDFS, the client contacts the <em>Namenode</em>, a computer that
serves as the gateway to the HDFS. It also performs a lot of administrative
tasks, such as saving the mapping between a file and the location of its block
across the cluster and so on. The Namenode tells the client which Datanodes (the
computers that make up the HDFS) to store the data onto. It also tells the
client which Datanodes to read the data from when a read request is
performed. See (<a class="reference internal" href="#hadoopschematic"><em>A schematic of the Hadoop File System</em></a>) for an graphical outline of the file
copy operation to the HDFS.</p>
<div class="figure align-center" id="hadoopschematic">
<img alt="_images/hdfs-write.pdf" src="_images/hdfs-write.pdf" />
<p class="caption">A schematic of the Hadoop File System</p>
</div>
</div>
<div class="section" id="hadoop-mapreduce">
<h2>Hadoop MapReduce<a class="headerlink" href="#hadoop-mapreduce" title="Permalink to this headline">¶</a></h2>
<p>Concurrent programming is difficult to get right. As Herb Sutter put it:</p>
<blockquote>
<div>... humans are quickly overwhelmed by concurrency and find it much more difficult to reason about concurrent than sequential code.</div></blockquote>
<p>A statistician attempting concurrent programming needs to be aware of race
conditions, deadlocks and tools to prevent this: locks, semaphores, and
mutually exclusive regions etc. An approach suggested by Sutter et al (<a class="reference internal" href="#stla">[STLa]</a>) is to
provide programming models not functions that force
the programmer to approach her algorithms differently. Once the programmer
constructs the algorithm using this model, concurrency comes for free.  The
MapReduce programming model is one example. Correctly coded Condor DAGS are another
example.</p>
<table class="docutils citation" frame="void" id="stla" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[STLa]</a></td><td>Software and the concurrency revolution, H. Sutter  and J. Larus, <em>ACM Queue, Volume 3,Number 7 2005</em></td></tr>
</tbody>
</table>
<p>MapReduce (<a class="reference internal" href="#mapred">[MapRed]</a>) consists of several embarrassingly parallel splits which are evaluated
in parallel. This is called the Map. There is a synchronization guard where
intermediate data created at the end of the Map is exchanged between nodes and
another round of parallel computing starts, called the Reduce phase. In effect
large scale simulation trials in which the programmer launches several thousands
of independent computations is an example of a Map. Retrieving and collating the
results (usually done in the R console) is an example of a manual reduce.</p>
<table class="docutils citation" frame="void" id="mapred" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[MapRed]</a></td><td>MapReduce: Simplified Data Processing on Large Clusters, Jeffrey Dean and Sanjay Ghemawat,*Communications of the ACM*, 2008</td></tr>
</tbody>
</table>
<p>In detail, the input to a MapReduce computation is a set of <em>N</em> <em>key,value</em>
pairs. The <em>N</em> pairs are partitioned into <em>S</em> arbitrary <em>splits</em>. Each split is
a unit of computation and is assigned to one computing unit on the cluster. Thus
the processing of the <em>S</em> splits occurs in parallel. Each split is processed by
a user given function <em>M</em>, that takes a sequence of input key,value pairs and outputs
(one or many) intermediate key,value pairs. The Hadoop framework will partition
the intermediate values by the intermediate key. That is intermediate values
sharing the same intermediate key are grouped together. Once the map is
complete, the if there are <em>M</em> distinct intermediate keys, a user given function
<em>R</em>, will be given an intermediate key and all intermediate values associated
with the same key. Each processing core is assigned a subset of intermediate
keys to reduce and the reduction of the <em>M</em> intermediate keys occurs in
parallel. The function <em>R</em>, takes an intermediate key, a stream of associated
intermediate values and returns a final key,value pair or pairs.</p>
<p>The R programmer has used MapReduce ideas. For example, the <tt class="docutils literal"><span class="pre">tapply</span></tt> command
splits a vector by a list of factors. This the map equivalent: each row of the
vector is the value and the keys are the distinct levels of the list of
factors. The reduce is the user given function applied to the partitions of the
vector. The <tt class="docutils literal"><span class="pre">xyplot</span></tt> function in <tt class="docutils literal"><span class="pre">lattice</span></tt> takes a formula e.g. <img class="math" src="_images/math/8003da33630b684cbe2d56d8a8235f6dbad749a2.png" alt="F\sim
Y|A*B"/>, subsets the the data frame by the cartesian product of the levels of
<img class="math" src="_images/math/019e9892786e493964e145e7c5cf7b700314e53b.png" alt="A"/> and <img class="math" src="_images/math/ff5fb3d775862e2123b007eb4373ff6cc1a34d4e.png" alt="B"/> (the map) and displays each subset (the reduce). Hadoop
MapReduce generalizes this to a distributed level.</p>
<div class="figure align-center">
<img alt="_images/mapredslov.pdf" src="_images/mapredslov.pdf" />
<p class="caption">An overview of Hadoop MapReduce</p>
</div>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>Two examples, one for quantiles and another for correlation will be described.</p>
<div class="section" id="approximate-quantile">
<h4>Approximate Quantile<a class="headerlink" href="#approximate-quantile" title="Permalink to this headline">¶</a></h4>
<p id="index-0">Let <img class="math" src="_images/math/6a47ca0fe7cb276abc022af6ac88ddae1a9d6894.png" alt="X"/> be a column of numbers. This can be arbitrarily large
(e.g. hundreds of gigabytes). The objective is to find the quantiles of the
<img class="math" src="_images/math/6a47ca0fe7cb276abc022af6ac88ddae1a9d6894.png" alt="X"/>. Let each number be a key. For discrete data e.g. ages (rounded to
years), count data, the number of unique numbers in a data set is generally not
large. For continuous data it can be many billions. In this case, we need to
discretize this. Care is needed before discretization. Discretions is equivalent
to binning and reduces the number of unique data points. For example, do not
round to the 5&#8217;th decimal place if the data points are the same for the first 5
decimal places!</p>
<p>For this example, let us assume the data is discrete (so no need for
rounding). The goal is to compute the frequency table of the data and use that
to compute the quantiles (see <a class="reference internal" href="#hynfan">[HynFan]</a>)</p>
<table class="docutils citation" frame="void" id="hynfan" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[HynFan]</a></td><td>Hyndman, R. J. and Fan, Y. (1996) &#8216;Sample quantiles in statistical packages&#8217;, <em>American Statistician</em>, <em>50</em>, 361-365).</td></tr>
</tbody>
</table>
<div class="highlight-r"><div class="highlight"><pre>for line in line_of_numbers:
   for number in tokenize line by <span class="p">[[</span>:space:<span class="p">]]</span>:
      write_key_value<span class="p">(</span>number<span class="p">,</span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The input data is partitioned into a splits of many lines of numbers. The above
code is applied to these splits in parallel. The intermediate keys are the
unique numbers and each has a list of <em>1</em>&#8216;s associated with it. Hadoop will sort
the keys by the number (not neccassirly by the quantity of the number, it
depends on the programming framework) and assign the aggregation computation of
the associated values for the different unique keys to different processing
cores in the reduce phase. The reduce logic is as</p>
<div class="highlight-r"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6</pre></div></td><td class="code"><div class="highlight"><pre>for number in stream_of_unique_numbers:
  sum<span class="o">=</span><span class="m">0</span>
  while has_more_values?<span class="p">()</span><span class="o">==</span><span class="kc">TRUE</span>:
    sum<span class="o">=</span>sum<span class="o">+</span>get_new_value<span class="p">()</span>
  end while
  write_key_value<span class="p">(</span>number<span class="p">,</span> sum<span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Notice the intermediate keys (the value of the number) and the final key (see
last line above) are the same. The unique numbers are partitioned. Thus the
stream in line 1 is stream of a subset. The different subsets are processed on
different compute cores. Note, the reduce code sums the <em>1</em>&#8216;s in a <em>while</em> loop
rather than loading them all into one gigantic array and adding the array. There
can be too many <em>1</em>&#8216;s to fit into core. This where the MapReduce implementation
shines: big data. The algorithm finally outputs the distinct numbers of
<img class="math" src="_images/math/6a47ca0fe7cb276abc022af6ac88ddae1a9d6894.png" alt="X"/> and the counts. This can be sorted and used to compute the
quantiles. This algorithm is also used to compute word frequencies for text
document analysis.</p>
</div>
<div class="section" id="correlation">
<h4>Correlation<a class="headerlink" href="#correlation" title="Permalink to this headline">¶</a></h4>
<p id="index-1">To compute the correlation of a text file of <em>N</em> rows and <em>C</em> columns, we need the
sum, sums of squares of each column and sum of unique pairs of columns. The
intermediate keys and final keys are the same: the column and column pair
identifiers. The value will be the sum of columns, their sum of squares,the
cross products and the number of entries.</p>
<p>We need to iterate over lines, tokenize, and compute the relevant column sums
and pairwise cross products.</p>
<div class="highlight-r"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10</pre></div></td><td class="code"><div class="highlight"><pre>for text_line in stream_of_text_lines
   tokenized_line <span class="o">=</span> tokenize text_line by <span class="p">[[</span>:space:<span class="p">]]</span>
   for i<span class="o">=</span><span class="m">1</span> to C:
      rhcollect<span class="p">(</span> <span class="p">(</span>i<span class="p">,</span>i<span class="p">),</span> <span class="p">(</span>n<span class="o">=</span><span class="m">1</span><span class="p">,</span>sum<span class="o">=</span>tokenized_line<span class="p">[</span>i<span class="p">],</span>ssq<span class="o">=</span>tokenized_line<span class="p">[</span>i<span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="p">))</span>
      for j<span class="o">=</span>i<span class="o">+</span><span class="m">1</span> to C:
          rhcollect<span class="p">(</span> <span class="p">(</span>i<span class="p">,</span>j<span class="p">),</span> <span class="p">(</span>crossprod<span class="o">=</span>
                           tokenized_line<span class="p">[</span>i<span class="p">]</span><span class="o">*</span> tokenized_line<span class="p">[</span>j<span class="p">]))</span>
      end for
    end for
end for
</pre></div>
</td></tr></table></div>
<p>The intermediate keys produced at the end of the map nodes are pairs
<img class="math" src="_images/math/887919dfbc86eebc29e0373f98f97dbf23a0ae23.png" alt="(i,j)"/> where <img class="math" src="_images/math/836d21cbafef0450788592f95a93c1c4805d1902.png" alt="1 \le i \le C, i \le j\le C"/> . The values are the
original value (the value for row <em>i</em> and column <em>j</em>), its square and
crossproduct. The <em>n</em> is just a <em>1</em>. By adding the values for this we obtain the
total number of rows. Inserting this <em>1</em> is wasteful, since it is redundantly
being passed around for all the keys - we could compute the number of rows in
another MapReduce job.</p>
<p>We need to sum this:</p>
<div class="highlight-r"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre>for identifier in stream_of_identifiers:
    <span class="c1">## identifier is a colum pair</span>
    sum <span class="o">=</span> empty tuple
    while has_more_values?<span class="p">()</span><span class="o">==</span><span class="kc">TRUE</span>:
        sum <span class="o">=</span> sum <span class="o">+</span> get_new_value<span class="p">()</span>
        <span class="c1"># get_new_value() will return</span>
        <span class="c1"># a tuple of length</span>
        <span class="c1">#     = 3 (if identifier is (i,i)</span>
        <span class="c1">#     = 1 (if identifier is (i,j) i &lt;&gt; j</span>
    end while
    rhcollect<span class="p">(</span>identifier<span class="p">,</span> sum<span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>The output will be a set of pairs or triplets according as the key is
<img class="math" src="_images/math/cc599cdd485f462626f08437fdbc4857bf4b4978.png" alt="(i,i)"/> or <img class="math" src="_images/math/887919dfbc86eebc29e0373f98f97dbf23a0ae23.png" alt="(i,j)"/>.</p>
</div>
<div class="section" id="computing-an-inner-join">
<h4>Computing an Inner Join<a class="headerlink" href="#computing-an-inner-join" title="Permalink to this headline">¶</a></h4>
<p id="index-2">We have a text file <strong>A</strong> of <em>N</em> columns and <strong>B</strong> with <em>M</em> columns. Both have a
common column <strong>index</strong>. In <strong>A</strong>, it is unique, with one row per level of
<strong>index</strong>. <strong>B</strong> is a repeated measurement data set, with the levels of
<strong>index</strong> repeated many times. <strong>B</strong> also contains a column called <strong>weight</strong>.
We need to compute the following operation, for every unique value of <strong>index</strong>
in <strong>A</strong>, compute the mean value of <strong>weight</strong>. We need to join the two data
sets on <strong>index</strong> and compute the number of observations and sum of <strong>weight</strong>
for unique values <strong>index</strong> and save a data set which consist of only those
values of <strong>index</strong> found in <strong>A</strong>.</p>
<p>In SQL, this is</p>
<div class="highlight-sql"><div class="highlight"><pre><span class="k">select</span>
       <span class="k">index</span><span class="p">,</span>
       <span class="n">mean</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span> <span class="k">as</span> <span class="n">mweight</span>
<span class="k">from</span> <span class="n">A</span> <span class="k">inner</span> <span class="k">join</span> <span class="n">B</span>
<span class="k">on</span> <span class="n">A</span><span class="p">.</span><span class="k">index</span><span class="o">=</span><span class="n">B</span><span class="p">.</span><span class="k">index</span>
<span class="k">group</span> <span class="k">by</span> <span class="n">A</span><span class="p">.</span><span class="k">index</span>
</pre></div>
</div>
<p>1. Compute the number of observations and sum of weights aggregated by levels of
<strong>index</strong> for <strong>B</strong>. This computes the values for all levels of <strong>index</strong> in <strong>B</strong> not just those found in <strong>A</strong>. Call this computed data set <strong>B&#8217;</strong></p>
<p>2. Merge <strong>B&#8217;</strong> and <strong>A</strong> .This might be wasteful since we compute for all levels in <strong>B</strong>, however if the
summarized data set <strong>B&#8217;</strong> will be used often, then this is a one time cost.</p>
<p>Summarizing <strong>B</strong> to <strong>B&#8217;</strong></p>
<div class="highlight-r"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13</pre></div></td><td class="code"><div class="highlight"><pre><span class="c1">#map</span>
for line in lines:
    index  <span class="o">=</span>  get column corresponding to <span class="s">&quot;index&quot;</span> from line
    weight <span class="o">=</span>  get column corresponding to <span class="s">&quot;weight&quot;</span> from line
    rhcollect<span class="p">(</span>index<span class="p">,(</span><span class="m">1</span><span class="p">,</span>weight<span class="p">))</span>

<span class="c1">#reduce</span>
for index in stream_of_indices:
    <span class="c1">## identifier is a colum pair</span>
    sum <span class="o">=</span> empty tuple
    while has_more_values?<span class="p">()</span><span class="o">==</span><span class="kc">TRUE</span>:
        sum <span class="o">=</span> sum <span class="o">+</span> get_new_value<span class="p">()</span>
    rhcollect<span class="p">(</span>index<span class="p">,</span> sum<span class="p">)</span> <span class="c1">#total number of obs, sum of weight</span>
</pre></div>
</td></tr></table></div>
<p>To merge, we map each index value found in <strong>A</strong> to a TRUE value and each value
found in <strong>B&#8217;</strong> to a tuple (number of observations and sum of weights). If a
value of <strong>index</strong> is found in <em>both</em> there will be two intermediate values for
that value of <strong>index</strong>. If instead the value of <strong>index</strong> exists in exactly one
of <strong>A</strong> and <strong>B</strong> there will be exactly one intermediate value. If there are
two values, one is a dummy (the TRUE) and the pseudo-code retains the value
whose length is 2 (the tuple).</p>
<div class="highlight-r"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></div></td><td class="code"><div class="highlight"><pre>for index in stream_of_indices:
    count <span class="o">=</span> <span class="m">0</span>
    information <span class="o">=</span> <span class="kc">NULL</span>
    while has_more_values?<span class="p">()</span><span class="o">==</span><span class="kc">TRUE</span>:
        count <span class="o">=</span> count <span class="o">+</span> <span class="m">1</span>
        temp <span class="o">=</span> get_next_value<span class="p">()</span>
        if length of temp <span class="o">==</span> <span class="m">2</span>:
            information <span class="o">=</span> temp
    end while
    if count <span class="o">==</span> <span class="m">2</span>:
        rhcollect<span class="p">(</span>index<span class="p">,</span> information<span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
</div>
<div class="section" id="combiners-an-optimization">
<h3>Combiners : An Optimization<a class="headerlink" href="#combiners-an-optimization" title="Permalink to this headline">¶</a></h3>
<p id="index-3">The Hadoop framework, sends all the intermediate values for a given key to the
reducer. The intermediate values for a given key are located on several compute
nodes and need to be shuffled (sent across the network) to the node assigned the
processing of that intermediate key. This involves a lot of network
transfer. Some operations do not need access to all of the data (intermediate
values) i.e they can compute on subsets and order does not matter i.e
associative and commutative operations. For example, the minimum of 8 numbers
<img class="math" src="_images/math/e6ba420cfb991b73347827999f55a14c5014535a.png" alt="min(x_1,x_2,\ldots,x_n) = min( min(x_1,x_2),min(x_3,\ldots,x_5),
min(x_6,\ldots,x_8))"/></p>
<p>The reduction occurs on just after the map phase on a subset of intermediate
values for a given intermediate keys. The output of this is sent to the
reducer. This greatly reduces network transfer and accelerates the job speed.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/combiner.png"><img alt="_images/combiner.png" src="_images/combiner.png" /></a>
<p class="caption">A MapReduce job using a combiner (the minimum operator). We consider   intermediate values for a single key.</p>
</div>
<p>The examples in this section outlined some algorithms that work with
MapReduce. Using RHIPE, there are ways to optimize the above code e.g. instead
of processing one line at a time use vector operations. Also, RHIPE calls the
code with R lists containing the input the keys and values. The streams in the
Reduce are replaced by lists of intermediate values and the R code is called
repeatedly with the list filled with new elements. This will be explained in the
Airline data example (see <em class="xref std std-ref">Airline Dataset</em>)</p>
</div>
</div>
<div class="section" id="r-and-hadoop-integrated-programming-environment">
<h2>R and Hadoop Integrated Programming Environment<a class="headerlink" href="#r-and-hadoop-integrated-programming-environment" title="Permalink to this headline">¶</a></h2>
<p>The R and Hadoop Integrated Programming Environment is R package to compute
across massive data sets, create subsets, apply routines to subsets, produce
displays on subsets across a cluster of computers using the Hadoop DFS and
Hadoop MapReduce framework. This is accomplished from within the R environment,
using standard R programming idioms. For efficiency reasons, the programming
style is slightly different from that outlined in the previous section.</p>
<p>The native language of Hadoop is Java. Java is not suitable for rapid
development such as is needed for a data analysis environment. <a class="reference external" href="http://hadoop.apache.org/common/docs/current/streaming.html">Hadoop Streaming</a>
bridges this gap. Users can write MapReduce programs in other languages
e.g. Python, Ruby, Perl which is then deployed over the cluster. Hadoop
Streaming then transfers the input data from Hadoop to the user program and vice
versa.</p>
<p>Data analysis from R does not involve the user writing code to be deployed from
the command line. The analyst has massive data sitting in the background, she
needs to create data, partition the data, compute summaries or displays. This
need to be evaluated from the R environment and the results returned to
R. Ideally not having to resort to the command line.</p>
<p>RHIPE is just that.</p>
<ul class="simple">
<li>RHIPE consist of several functions to interact with the HDFS e.g. save data
sets, read data created by RHIPE MapReduce, delete files.</li>
<li>Compose and launch MapReduce jobs from R using the command <tt class="docutils literal"><span class="pre">rhmr</span></tt> and
<tt class="docutils literal"><span class="pre">rhex</span></tt>. Monitor the status using <tt class="docutils literal"><span class="pre">rhstatus</span></tt> which returns an R
object. Stop jobs using <tt class="docutils literal"><span class="pre">rhkill</span></tt></li>
<li>Compute <em>side effect</em> files. The output of parallel computations may include
the creation of PDF files, R data sets, CVS files etc. These will be copied by
RHIPE to a central location on the HDFS removing the need for the user to copy
them from the compute nodes or setting up a network file system.</li>
<li>Data sets that are created by RHIPE can be read using other languages such as
Java, Perl, Python and C. The serialization format used by RHIPE (converting R
objects to binary data) uses Googles <a class="reference external" href="http://code.google.com/p/protobuf/">Protocol Buffers</a> which is very fast and creates compact
representations for R objects. Ideal for massive data sets.</li>
<li>Data sets created using RHIPE are <em>key-value</em> pairs. A key is mapped to a
value. A MapReduce computations iterates over the key,value pairs in
parallel. If the output of a RHIPE job creates unique keys the output can be
treated as a external-memory associative dictionary. RHIPE can thus be used as
a medium scale (millions of keys) disk based dictionary, which is useful for
loading R objects into R.</li>
</ul>
<p id="index-4"><strong>Future Work</strong></p>
<blockquote>
<div>I plan on incorporating input and output bridges between RHIPE and
HBase.</div></blockquote>
<p>In summary, the objective of RHIPE is to let the user focus on thinking about
the data. The difficulties in distributing computations and storing data across
a cluster are automatically handled by RHIPE and Hadoop.</p>
</div>
</div>


      </div>
      <div class="bottomnav">
      
        <p>
        «&#160;&#160;<a href="installation.html">Installation</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="airline.html">Airline Dataset</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer">
        &copy; Copyright 2010, Saptarshi Guha.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.2.
    </div>
  </body>
</html>